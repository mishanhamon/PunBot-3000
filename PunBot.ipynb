{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c379de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary packages for the PunBot\n",
    "from collections import Counter\n",
    "import spacy\n",
    "word2vec = spacy.load('en_core_web_lg')\n",
    "import csv\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "# this package allows PunBot to be integrated with telegram\n",
    "import telebot\n",
    "\n",
    "# importing pre-written functions for entity recognition and response selection \n",
    "from user_functions import preprocess, compare_overlap, pos_tag, extract_nouns, compute_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd0b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is borrowed from the 2020 github publication by Kamran Janjua available at https://github.com/kjanjua26/Pyphones/tree/master\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List\n",
    "import re\n",
    "\n",
    "class Pyphones:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.url = \"https://www.homophone.com/search?page={}&type=&q={}\"\n",
    "        self.homophones = {self.word: []}\n",
    "        \n",
    "    def get_the_page(self, page_no=1):\n",
    "        \"\"\"\n",
    "        Get the page content.\n",
    "\n",
    "        Returns\n",
    "            str: the content of the page.\n",
    "        \"\"\"\n",
    "        url = self.url.format(page_no, self.word)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        return soup\n",
    "\n",
    "    def get_the_page_nos(self):\n",
    "        \"\"\"\n",
    "        Get the total number of pages\n",
    "\n",
    "        Returns\n",
    "            int: the total number of the pages.\n",
    "        \"\"\"\n",
    "        soup = self.get_the_page()\n",
    "        pages = soup.find_all('div', attrs={'class':'col-sm-9'})\n",
    "        total_pages = pages[0].find('h5').text.split('/')[-1].strip()\n",
    "        return int(total_pages)\n",
    "    \n",
    "\n",
    "    def get_the_homophones(self):\n",
    "        \"\"\"\n",
    "        Get the homophones of the word.\n",
    "\n",
    "        Returns\n",
    "            dict: {word: [list_of_homophones]} against each word.\n",
    "        \"\"\"\n",
    "        total_pages = self.get_the_page_nos()\n",
    "        for ix in range(total_pages):\n",
    "            page_no = ix + 1\n",
    "            soup = self.get_the_page(page_no)\n",
    "            raw_homophones = soup.find_all('div', attrs={'class': 'well well-lg'})\n",
    "            for elem in range(len(raw_homophones)):\n",
    "                raw_homophones_2 = raw_homophones[elem].find_all('a', attrs={'class': 'btn word-btn'})\n",
    "                list_of_homophones = list(raw_homophones_2)\n",
    "                if any(list_of_homophones):\n",
    "                    local_homophones = []\n",
    "                    for tag_of_homophone in list_of_homophones:\n",
    "                        homophone = tag_of_homophone.text\n",
    "                        local_homophones.append(homophone)\n",
    "                    self.homophones[self.word].append(local_homophones)\n",
    "                    \n",
    "    \n",
    "\n",
    "        return self.homophones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cf3c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed if they start with a verb. Filtered idioms saved to filtered_idioms.csv\n"
     ]
    }
   ],
   "source": [
    "# I use a database of idioms provided by zaghloul404 (2023), available at https://github.com/zaghloul404/englishidioms/tree/main\n",
    "# In this section, I remove artifacts from idioms and remove idioms that start with a verb \n",
    "\n",
    "def starts_with_verb(idiom):\n",
    "    first_word = idiom.split()[0]\n",
    "    synsets = wordnet.synsets(first_word)\n",
    "    for synset in synsets:\n",
    "        if synset.pos() == 'v':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_rows_with_verbs_or_brackets(file_path):\n",
    "    # Open the CSV file\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "\n",
    "        filtered_rows = []\n",
    "        for row in reader:\n",
    "            row[2] = row[2].replace(\"â€ \", \"\").replace(\"*\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            if not starts_with_verb(row[2]):\n",
    "                filtered_rows.append(row)\n",
    "\n",
    "    return header, filtered_rows\n",
    "\n",
    "# Applying the functions to the idioms\n",
    "file_path = \"idioms.csv\" \n",
    "header, filtered_rows = subtitute_brackets(file_path)\n",
    "\n",
    "# Write the filtered rows to a new CSV file\n",
    "with open(\"filtered_idioms.csv\", 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header) \n",
    "    writer.writerows(filtered_rows)\n",
    "\n",
    "print(\"Rows removed if they start with a verb. Filtered idioms saved to filtered_idioms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e58ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions capture grammatical variations in the idioms\n",
    "# This element aims to improve the quality of PunBot's responses\n",
    "\n",
    "# checking if the idiom contains a verb, starts with a starting word that leads to the same grammatical structure\n",
    "\n",
    "def contains_verb(idiom):\n",
    "    words = idiom.split()\n",
    "    common_starters = ['i', \"i've\", \"i'll\", \"you\", \"you'll\", \"you've\", 'if', \"there's\", \"that's\", \"there're\"]\n",
    "    if words and words[0].lower() in common_starters:\n",
    "        return True\n",
    "    \n",
    "    words = word_tokenize(idiom)\n",
    "    tagged_words = pos_tag(words)\n",
    "    for word, pos in tagged_words:\n",
    "        if pos.startswith('V'):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Check if the idiom starts with a not and contains a verb \n",
    "def starts_with_not_verb(idiom):\n",
    "        words = idiom.split()\n",
    "        if words and words[0].lower() == 'not':\n",
    "            words = word_tokenize(idiom)\n",
    "            tagged_words = pos_tag(words)\n",
    "            for word, pos in tagged_words:\n",
    "                if pos.startswith('V'):\n",
    "                    return True\n",
    "        return False    \n",
    "\n",
    "# Check if the idiom starts with cannot or doesn't\n",
    "def starts_with_cannot_verb(idiom):\n",
    "    words = idiom.split()\n",
    "    common_starters = ['cannot', \"doesn't\", \"don't\"]\n",
    "    if words and words[0].lower() in common_starters:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    " # Check if the idiom starts with a\n",
    "def starts_with_a(idiom):\n",
    "        words = idiom.split()\n",
    "        \n",
    "        if words and words[0].lower() == 'a':\n",
    "            words = word_tokenize(idiom)\n",
    "            tagged_words = pos_tag(words)\n",
    "            for word, pos in tagged_words:\n",
    "                if not pos.startswith('V'):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "# Check if the idiom containts a to  \n",
    "def contains_to(idiom):\n",
    "    if \"to\" in idiom:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e16d0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I define all fixed elements\n",
    "\n",
    "# These commands ensure that PunBot does not generate response when the user wants to \n",
    "# leave the chat or expresses satisfaction with the pun\n",
    "\n",
    "exit_commands = (\"quit\", \"goodbye\", \"exit\", \"bye\", 'no', 'do not')\n",
    "positive_commands = (\"yes\", \"funny\", \"fun\", \"hilarious\", 'thank', 'super', 'of course', 'sure')\n",
    "hi_commands = (\"hi\", \"hello\")\n",
    "\n",
    "# These pre-defined responses are aimed to tailor for the specific requests by the user\n",
    "response_default = \"Here is one hilarious joke about {} \"\n",
    "response_a = \"Of course I have a provokative pun about {} \"\n",
    "response_b = \"I agree, {} is a great topic for a dirty pun\"\n",
    "response_c = \"My father once told me a nice silly pun about {}\"\n",
    "response_d = \"My grandfather once told me a nice dark humor pun about {}\"\n",
    "response_e = \"Ow, I have plenty dummy puns on {}\"\n",
    "response_f = \"Your offensive pun on {} is being generated\"\n",
    "\n",
    "# 'Object' tends to work the best for entity recognition \n",
    "blank_spot = \"object\"\n",
    "\n",
    "# Loading the filtered idioms\n",
    "idiom_file = \"filtered_idioms.csv\"\n",
    "\n",
    "# The first part of the pun is always the same question that takes into user-specific entity\n",
    "# and a random adjective\n",
    "question = \"Question: Why is the {} so {}?\"\n",
    "\n",
    "# Although adjectives have no baring on the content of the pun, they add some \n",
    "# randomness and thus contribute to the comic effect\n",
    "states = [ 'happy', 'sad', 'tired', 'worried', 'sleepy', 'angry', 'smart', 'excited']\n",
    "\n",
    "# The second part of the pun is an idiom-specific answer that takes into a modified idiom\n",
    "# Answers are desigend to capture grammatical variation in idioms\n",
    "answer_no_verb = \"Answer: Because it is {}\"\n",
    "answer_verb = \"Answer: Because {}\"\n",
    "answer_verb_not = \"Answer: Because it is/does {}\"\n",
    "answer_verb_cannot = \"Answer: Because it {}\"\n",
    "answer_a = \"Answer: Because of {}\"\n",
    "answer_to = \"Answer: Because it is {}\"\n",
    "\n",
    "responses = [response_default, response_a, response_b, response_c, response_d, response_e, response_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1db5990a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This section provides functions that allow PunBot to receive messages from the user and generate puns\n",
    "# The code is designed to work with Telegram\n",
    "\n",
    "# Initializing PunBot with an API token\n",
    "bot = telebot.TeleBot(\"#######\")\n",
    "\n",
    "# defining an exit function\n",
    "def make_exit(user_message):\n",
    "    for object in exit_commands: \n",
    "        if object in user_message: \n",
    "            return True \n",
    "    return False\n",
    "\n",
    "# The idiom finder\n",
    "def find_idioms(entity):\n",
    "\n",
    "    idioms_with_entity = []\n",
    "\n",
    "    with open(idiom_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader) \n",
    "        for row in reader:\n",
    "            idiom = row[2].strip() \n",
    "            if entity in idiom:\n",
    "                idioms_with_entity.append(idiom)\n",
    "\n",
    "    if idioms_with_entity:\n",
    "        random_idiom = random.choice(idioms_with_entity)\n",
    "        modified_idiom = random_idiom.replace(entity, '{}')\n",
    "        return modified_idiom.lower()\n",
    "    else:\n",
    "        return False  \n",
    "\n",
    "\n",
    "    \n",
    "# The homophone replacement\n",
    "def replace_with_homophones(entity):\n",
    "    py = Pyphones(entity)\n",
    "    try:\n",
    "        homophones = py.get_the_homophones()\n",
    "    except IndexError:\n",
    "        return False\n",
    "    \n",
    "    if homophones is not False:\n",
    "        words = entity.split()\n",
    "        for i in range(len(words)):\n",
    "            word = words[i].lower()\n",
    "            if word in homophones:\n",
    "                homophone_lists = homophones[word]\n",
    "                for homophone_list in homophone_lists:\n",
    "                    if homophone_list:\n",
    "                        selected_homophone = random.choice(homophone_list)\n",
    "                        words[i] = selected_homophone\n",
    "        modified_entity = ' '.join(words)\n",
    "        return modified_entity\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Findging intent\n",
    "def find_intent_match(responses, user_message):\n",
    "    bow_user_message = Counter(preprocess(user_message))\n",
    "    processed_responses = [Counter(preprocess(response)) for response in responses]\n",
    "    similarity_list = [compare_overlap(doc, bow_user_message) for doc in processed_responses]\n",
    "    response_index = similarity_list.index(max(similarity_list))\n",
    "    return responses[response_index]\n",
    "\n",
    "# Finding entities\n",
    "def find_entities(user_message):\n",
    "    \n",
    "    # replace 'pun' in the message to avoid confusion \n",
    "    user_message = user_message.replace('pun', '').replace('please', '')\n",
    "    \n",
    "    tagged_user_message = pos_tag(preprocess(user_message))\n",
    "    message_nouns = extract_nouns(tagged_user_message)\n",
    "    tokens = word2vec(\" \".join(message_nouns))\n",
    "    category = word2vec(blank_spot)\n",
    "    word2vec_result = compute_similarity(tokens, category)\n",
    "    word2vec_result.sort(key=lambda x: x[2])\n",
    "    if len(word2vec_result) < 1:\n",
    "        return blank_spot\n",
    "    else:\n",
    "        return word2vec_result[-1][0]\n",
    "\n",
    "# The respond function \n",
    "def respond(user_message):\n",
    "    \n",
    "    best_response = find_intent_match(responses, user_message)\n",
    "    entity = find_entities(user_message)\n",
    "    response_text = best_response.format(entity)\n",
    "\n",
    "    random_state = random.choice(states)\n",
    "    \n",
    "    question_text = question.format(entity, random_state)\n",
    "\n",
    "    if replace_with_homophones(entity) is not False: \n",
    "        pun_entity = replace_with_homophones(entity)\n",
    "    else: \n",
    "        pun_entity = entity\n",
    "\n",
    "    idiom = find_idioms(entity)\n",
    "\n",
    "    if idiom is False: \n",
    "        return \"Sorry.. can't think of any pun on that actually...\"\n",
    "\n",
    "    \n",
    "    else: \n",
    "        idiom_punned = idiom.format(pun_entity)\n",
    "        \n",
    "        answer_text = answer_no_verb.format(idiom_punned)\n",
    "\n",
    "        # using different answers for different grammar structures\n",
    "        if contains_verb(idiom): \n",
    "            answer_text = answer_verb.format(idiom_punned)\n",
    "        if starts_with_a(idiom):\n",
    "            answer_text = answer_a.format(idiom_punned)\n",
    "        if starts_with_cannot_verb(idiom):\n",
    "            answer_text = answer_verb_cannot.format(idiom_punned)\n",
    "        if contains_to(idiom):\n",
    "            answer_text = answer_to.format(idiom_punned)\n",
    "            \n",
    "            \n",
    "        response_text = \"{}\\n\\n{}\\n{}\".format(response_text, question_text, answer_text)\n",
    "        \n",
    "        idiom_punned = ''\n",
    "        idiom = ''\n",
    "        entity = ''\n",
    "        \n",
    "        return response_text\n",
    "    \n",
    "        \n",
    "\n",
    "# Define a Telegram handler for the start command\n",
    "@bot.message_handler(commands=['start'])\n",
    "def send_welcome(message):\n",
    "    bot.send_message(message.chat.id, \"Welcome to Pun Bot! Would you prefer a silly, dirty or a provocative pun? Also, what should be the topic of the pun?\")\n",
    "\n",
    "# Define a Telegram handler for incoming messages\n",
    "@bot.message_handler(func=lambda message: True)\n",
    "def handle_message(message):\n",
    "    user_message = message.text.lower()\n",
    "    \n",
    "    found_exit_command = False\n",
    "    for command in exit_commands:\n",
    "        if command in user_message:\n",
    "            found_exit_command = True\n",
    "            bot.send_message(message.chat.id, \"Oki-doki. See you later, alligator!\")\n",
    "            break\n",
    "            \n",
    "    found_hi_command = False\n",
    "    for command in hi_commands:\n",
    "        words = user_message.split()\n",
    "        if command in user_message.lower() and len(words) < 3:\n",
    "            found_hi_command = True\n",
    "            bot.send_message(message.chat.id, \"Hi! Wanna hear a pun?\")\n",
    "            break\n",
    "    \n",
    "    found_positive_command = False\n",
    "    for command in positive_commands:  # corrected variable name\n",
    "        if command in user_message:\n",
    "            found_positive_command = True\n",
    "            bot.send_message(message.chat.id, \"Thanks! Please specify the topic for another ridiculous pun!\")\n",
    "            break\n",
    "                \n",
    "    if not found_positive_command and not found_exit_command and not found_hi_command:\n",
    "            response_text = respond(user_message)\n",
    "            bot.reply_to(message, response_text)\n",
    "            bot.send_message(message.chat.id, \"How about another one?\")\n",
    "\n",
    "# Polling for messages\n",
    "bot.polling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
